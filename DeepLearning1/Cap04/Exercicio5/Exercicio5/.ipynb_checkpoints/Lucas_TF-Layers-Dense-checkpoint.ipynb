{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:25:20.787927Z",
     "start_time": "2020-12-27T11:25:18.026191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "import tensorflow \n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:25:58.365063Z",
     "start_time": "2020-12-27T11:25:58.362317Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:45.522123Z",
     "start_time": "2020-12-27T11:30:45.230395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/lucas/Documents/ia-lucas-dados/dados/DeepLearningI/Cap04/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting /home/lucas/Documents/ia-lucas-dados/dados/DeepLearningI/Cap04/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/lucas/Documents/ia-lucas-dados/dados/DeepLearningI/Cap04/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/lucas/Documents/ia-lucas-dados/dados/DeepLearningI/Cap04/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/home/lucas/Documents/ia-lucas-dados/dados/DeepLearningI/Cap04/MNIST\", \n",
    "                                 one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:32:40.616151Z",
     "start_time": "2020-12-27T11:32:40.609241Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "learning_rate = 0.1 \n",
    "training_epochs = 10 \n",
    "batch_size = 64 \n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 128 \n",
    "n_hidden_2 = 256 \n",
    "n_input = 784 \n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:41:25.963945Z",
     "start_time": "2020-12-27T11:41:25.873640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining Graph\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    #Input Data\n",
    "    tf_x = tf.compat.v1.placeholder(tf.float32, [None, n_input], name='features')\n",
    "    tf_y = tf.compat.v1.placeholder(tf.float32, [None, n_classes], name=\"targets\")\n",
    "    \n",
    "    # Multilayer Perceptron\n",
    "    layer_1 = tf.compat.v1.layers.dense(tf_x, n_hidden_1,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer_2 = tf.compat.v1.layers.dense(layer_1, n_hidden_2,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    out_layer = tf.compat.v1.layers.dense(layer_2, n_classes, activation=None)\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=out_layer, labels=tf.stop_gradient(tf_y))\n",
    "    cost = tf.reduce_mean(input_tensor=loss, name='cost')\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "    \n",
    "    # Predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(input=tf_y, axis=1), tf.argmax(input=out_layer, axis=1))\n",
    "    accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:52:12.279276Z",
     "start_time": "2020-12-27T11:52:10.425130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | AvgCostL 1.132| Acuracia em Treino/Validacao: 0.842/0.850\n",
      "Epoch 002 | AvgCostL 0.486| Acuracia em Treino/Validacao: 0.873/0.882\n",
      "Epoch 003 | AvgCostL 0.375| Acuracia em Treino/Validacao: 0.897/0.906\n",
      "Epoch 004 | AvgCostL 0.350| Acuracia em Treino/Validacao: 0.897/0.906\n",
      "Epoch 005 | AvgCostL 0.354| Acuracia em Treino/Validacao: 0.908/0.913\n",
      "Epoch 006 | AvgCostL 0.282| Acuracia em Treino/Validacao: 0.919/0.921\n",
      "Epoch 007 | AvgCostL 0.273| Acuracia em Treino/Validacao: 0.915/0.921\n",
      "Epoch 008 | AvgCostL 0.267| Acuracia em Treino/Validacao: 0.929/0.930\n",
      "Epoch 009 | AvgCostL 0.255| Acuracia em Treino/Validacao: 0.932/0.936\n",
      "Epoch 010 | AvgCostL 0.251| Acuracia em Treino/Validacao: 0.936/0.938\n",
      "Acuracia em teste 0.935\n"
     ]
    }
   ],
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "with tf.compat.v1.Session(graph=g) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0. \n",
    "        total_batch = mnist.train.num_examples // batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run(['train', 'cost:0'],\n",
    "                           feed_dict = {\n",
    "                               'features:0': batch_x,\n",
    "                               'targets:0': batch_y\n",
    "                           })\n",
    "            avg_cost += c \n",
    "            \n",
    "        train_acc = sess.run('accuracy:0', feed_dict={\n",
    "            'features:0': mnist.train.images,\n",
    "            'targets:0': mnist.train.labels\n",
    "        })\n",
    "        \n",
    "        valid_acc = sess.run('accuracy:0', feed_dict={\n",
    "            'features:0': mnist.validation.images,\n",
    "            'targets:0': mnist.validation.labels\n",
    "        })\n",
    "            \n",
    "        print(\"Epoch %03d | AvgCostL %.3f\" % (epoch + 1, avg_cost/(i + 1)), end=\"\")\n",
    "        print(\"| Acuracia em Treino/Validacao: %.3f/%.3f\" % (train_acc, valid_acc))\n",
    "        \n",
    "    test_acc = sess.run('accuracy:0', feed_dict={\n",
    "        'features:0': mnist.test.images,\n",
    "        'targets:0': mnist.test.labels\n",
    "    })\n",
    "    print(\"Acuracia em teste %.3f\" %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:55:21.600130Z",
     "start_time": "2020-12-27T11:55:17.735060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | AvgCostL 0.866| Acuracia em Treino/Validacao: 0.865/0.868\n",
      "Epoch 002 | AvgCostL 0.380| Acuracia em Treino/Validacao: 0.906/0.911\n",
      "Epoch 003 | AvgCostL 0.305| Acuracia em Treino/Validacao: 0.924/0.928\n",
      "Epoch 004 | AvgCostL 0.269| Acuracia em Treino/Validacao: 0.901/0.903\n",
      "Epoch 005 | AvgCostL 0.251| Acuracia em Treino/Validacao: 0.934/0.935\n",
      "Epoch 006 | AvgCostL 0.235| Acuracia em Treino/Validacao: 0.944/0.946\n",
      "Epoch 007 | AvgCostL 0.190| Acuracia em Treino/Validacao: 0.944/0.944\n",
      "Epoch 008 | AvgCostL 0.196| Acuracia em Treino/Validacao: 0.951/0.951\n",
      "Epoch 009 | AvgCostL 0.221| Acuracia em Treino/Validacao: 0.955/0.952\n",
      "Epoch 010 | AvgCostL 0.187| Acuracia em Treino/Validacao: 0.948/0.947\n",
      "Acuracia em teste 0.946\n"
     ]
    }
   ],
   "source": [
    "# Alter the multilayerperceptron\n",
    "\n",
    "# Increase length of hidden_layers\n",
    "\n",
    "#Hyperparams\n",
    "learning_rate = 0.1 \n",
    "training_epochs = 10 \n",
    "batch_size = 64 \n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 512 \n",
    "n_hidden_2 = 512 \n",
    "n_input = 784 \n",
    "n_classes = 10\n",
    "\n",
    "# Defining Graph\n",
    "\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    \n",
    "    #Input Data\n",
    "    tf_x = tf.compat.v1.placeholder(tf.float32, [None, n_input], name='features')\n",
    "    tf_y = tf.compat.v1.placeholder(tf.float32, [None, n_classes], name=\"targets\")\n",
    "    \n",
    "    # Multilayer Perceptron\n",
    "    layer_1 = tf.compat.v1.layers.dense(tf_x, n_hidden_1,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer_2 = tf.compat.v1.layers.dense(layer_1, n_hidden_2,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    out_layer = tf.compat.v1.layers.dense(layer_2, n_classes, activation=None)\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=out_layer, labels=tf.stop_gradient(tf_y))\n",
    "    cost = tf.reduce_mean(input_tensor=loss, name='cost')\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "    \n",
    "    # Predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(input=tf_y, axis=1), tf.argmax(input=out_layer, axis=1))\n",
    "    accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    \n",
    "# Training and Evaluation\n",
    "\n",
    "with tf.compat.v1.Session(graph=g1) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0. \n",
    "        total_batch = mnist.train.num_examples // batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run(['train', 'cost:0'],\n",
    "                           feed_dict = {\n",
    "                               'features:0': batch_x,\n",
    "                               'targets:0': batch_y\n",
    "                           })\n",
    "            avg_cost += c \n",
    "            \n",
    "        train_acc = sess.run('accuracy:0', feed_dict={\n",
    "            'features:0': mnist.train.images,\n",
    "            'targets:0': mnist.train.labels\n",
    "        })\n",
    "        \n",
    "        valid_acc = sess.run('accuracy:0', feed_dict={\n",
    "            'features:0': mnist.validation.images,\n",
    "            'targets:0': mnist.validation.labels\n",
    "        })\n",
    "            \n",
    "        print(\"Epoch %03d | AvgCostL %.3f\" % (epoch + 1, avg_cost/(i + 1)), end=\"\")\n",
    "        print(\"| Acuracia em Treino/Validacao: %.3f/%.3f\" % (train_acc, valid_acc))\n",
    "        \n",
    "    test_acc = sess.run('accuracy:0', feed_dict={\n",
    "        'features:0': mnist.test.images,\n",
    "        'targets:0': mnist.test.labels\n",
    "    })\n",
    "    print(\"Acuracia em teste %.3f\" %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:57:43.408736Z",
     "start_time": "2020-12-27T11:57:38.366585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | AvgCostL 0.900| Acuracia em Treino/Validacao: 0.885/0.892\n",
      "Epoch 002 | AvgCostL 0.392| Acuracia em Treino/Validacao: 0.886/0.892\n",
      "Epoch 003 | AvgCostL 0.301| Acuracia em Treino/Validacao: 0.923/0.920\n",
      "Epoch 004 | AvgCostL 0.269| Acuracia em Treino/Validacao: 0.937/0.935\n",
      "Epoch 005 | AvgCostL 0.212| Acuracia em Treino/Validacao: 0.942/0.937\n",
      "Epoch 006 | AvgCostL 0.213| Acuracia em Treino/Validacao: 0.948/0.945\n",
      "Epoch 007 | AvgCostL 0.167| Acuracia em Treino/Validacao: 0.954/0.949\n",
      "Epoch 008 | AvgCostL 0.176| Acuracia em Treino/Validacao: 0.953/0.950\n",
      "Epoch 009 | AvgCostL 0.167| Acuracia em Treino/Validacao: 0.957/0.954\n",
      "Epoch 010 | AvgCostL 0.164| Acuracia em Treino/Validacao: 0.965/0.965\n",
      "Acuracia em teste 0.957\n"
     ]
    }
   ],
   "source": [
    "# Alter the multilayerperceptron\n",
    "\n",
    "# Adding one more hidden_layers\n",
    "\n",
    "#Hyperparams\n",
    "learning_rate = 0.1 \n",
    "training_epochs = 10 \n",
    "batch_size = 64 \n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 512 \n",
    "n_hidden_2 = 512 \n",
    "n_hidden_3 = 512\n",
    "n_input = 784 \n",
    "n_classes = 10\n",
    "\n",
    "# Defining Graph\n",
    "\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    \n",
    "    #Input Data\n",
    "    tf_x = tf.compat.v1.placeholder(tf.float32, [None, n_input], name='features')\n",
    "    tf_y = tf.compat.v1.placeholder(tf.float32, [None, n_classes], name=\"targets\")\n",
    "    \n",
    "    # Multilayer Perceptron\n",
    "    layer_1 = tf.compat.v1.layers.dense(tf_x, n_hidden_1,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer_2 = tf.compat.v1.layers.dense(layer_1, n_hidden_2,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer_3 = tf.compat.v1.layers.dense(layer_2, n_hidden_3,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    out_layer = tf.compat.v1.layers.dense(layer_3, n_classes, activation=None)\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=out_layer, labels=tf.stop_gradient(tf_y))\n",
    "    cost = tf.reduce_mean(input_tensor=loss, name='cost')\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "    \n",
    "    # Predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(input=tf_y, axis=1), tf.argmax(input=out_layer, axis=1))\n",
    "    accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    \n",
    "# Training and Evaluation\n",
    "\n",
    "with tf.compat.v1.Session(graph=g1) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0. \n",
    "        total_batch = mnist.train.num_examples // batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run(['train', 'cost:0'],\n",
    "                           feed_dict = {\n",
    "                               'features:0': batch_x,\n",
    "                               'targets:0': batch_y\n",
    "                           })\n",
    "            avg_cost += c \n",
    "            \n",
    "        train_acc = sess.run('accuracy:0', feed_dict={\n",
    "            'features:0': mnist.train.images,\n",
    "            'targets:0': mnist.train.labels\n",
    "        })\n",
    "        \n",
    "        valid_acc = sess.run('accuracy:0', feed_dict={\n",
    "            'features:0': mnist.validation.images,\n",
    "            'targets:0': mnist.validation.labels\n",
    "        })\n",
    "            \n",
    "        print(\"Epoch %03d | AvgCostL %.3f\" % (epoch + 1, avg_cost/(i + 1)), end=\"\")\n",
    "        print(\"| Acuracia em Treino/Validacao: %.3f/%.3f\" % (train_acc, valid_acc))\n",
    "        \n",
    "    test_acc = sess.run('accuracy:0', feed_dict={\n",
    "        'features:0': mnist.test.images,\n",
    "        'targets:0': mnist.test.labels\n",
    "    })\n",
    "    print(\"Acuracia em teste %.3f\" %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:58:35.920285Z",
     "start_time": "2020-12-27T11:58:25.989959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | AvgCostL 1.152| Acuracia em Treino/Validacao: 0.867/0.874\n",
      "Epoch 002 | AvgCostL 0.366| Acuracia em Treino/Validacao: 0.908/0.914\n",
      "Epoch 003 | AvgCostL 0.302| Acuracia em Treino/Validacao: 0.922/0.927\n",
      "Epoch 004 | AvgCostL 0.247| Acuracia em Treino/Validacao: 0.934/0.934\n",
      "Epoch 005 | AvgCostL 0.237| Acuracia em Treino/Validacao: 0.945/0.946\n",
      "Epoch 006 | AvgCostL 0.193| Acuracia em Treino/Validacao: 0.950/0.953\n",
      "Epoch 007 | AvgCostL 0.187| Acuracia em Treino/Validacao: 0.952/0.952\n",
      "Epoch 008 | AvgCostL 0.183| Acuracia em Treino/Validacao: 0.961/0.960\n",
      "Epoch 009 | AvgCostL 0.174| Acuracia em Treino/Validacao: 0.961/0.962\n",
      "Epoch 010 | AvgCostL 0.159| Acuracia em Treino/Validacao: 0.963/0.961\n",
      "Epoch 011 | AvgCostL 0.163| Acuracia em Treino/Validacao: 0.965/0.963\n",
      "Epoch 012 | AvgCostL 0.155| Acuracia em Treino/Validacao: 0.965/0.962\n",
      "Epoch 013 | AvgCostL 0.134| Acuracia em Treino/Validacao: 0.965/0.961\n",
      "Epoch 014 | AvgCostL 0.144| Acuracia em Treino/Validacao: 0.961/0.960\n",
      "Epoch 015 | AvgCostL 0.140| Acuracia em Treino/Validacao: 0.972/0.967\n",
      "Epoch 016 | AvgCostL 0.093| Acuracia em Treino/Validacao: 0.974/0.966\n",
      "Epoch 017 | AvgCostL 0.084| Acuracia em Treino/Validacao: 0.973/0.967\n",
      "Epoch 018 | AvgCostL 0.097| Acuracia em Treino/Validacao: 0.974/0.969\n",
      "Epoch 019 | AvgCostL 0.095| Acuracia em Treino/Validacao: 0.978/0.970\n",
      "Epoch 020 | AvgCostL 0.093| Acuracia em Treino/Validacao: 0.974/0.967\n",
      "Acuracia em teste 0.962\n"
     ]
    }
   ],
   "source": [
    "# Alter the multilayerperceptron\n",
    "\n",
    "# Adding one more hidden_layers\n",
    "\n",
    "#Hyperparams\n",
    "learning_rate = 0.1 \n",
    "training_epochs = 20 \n",
    "batch_size = 64 \n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 512 \n",
    "n_hidden_2 = 512 \n",
    "n_hidden_3 = 512\n",
    "n_input = 784 \n",
    "n_classes = 10\n",
    "\n",
    "# Defining Graph\n",
    "\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    \n",
    "    #Input Data\n",
    "    tf_x = tf.compat.v1.placeholder(tf.float32, [None, n_input], name='features')\n",
    "    tf_y = tf.compat.v1.placeholder(tf.float32, [None, n_classes], name=\"targets\")\n",
    "    \n",
    "    # Multilayer Perceptron\n",
    "    layer_1 = tf.compat.v1.layers.dense(tf_x, n_hidden_1,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer_2 = tf.compat.v1.layers.dense(layer_1, n_hidden_2,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer_3 = tf.compat.v1.layers.dense(layer_2, n_hidden_3,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    out_layer = tf.compat.v1.layers.dense(layer_3, n_classes, activation=None)\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=out_layer, labels=tf.stop_gradient(tf_y))\n",
    "    cost = tf.reduce_mean(input_tensor=loss, name='cost')\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "    \n",
    "    # Predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(input=tf_y, axis=1), tf.argmax(input=out_layer, axis=1))\n",
    "    accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    \n",
    "# Training and Evaluation\n",
    "\n",
    "with tf.compat.v1.Session(graph=g1) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0. \n",
    "        total_batch = mnist.train.num_examples // batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run(['train', 'cost:0'],\n",
    "                           feed_dict = {\n",
    "                               'features:0': batch_x,\n",
    "                               'targets:0': batch_y\n",
    "                           })\n",
    "            avg_cost += c \n",
    "            \n",
    "        train_acc = sess.run('accuracy:0', feed_dict={\n",
    "            'features:0': mnist.train.images,\n",
    "            'targets:0': mnist.train.labels\n",
    "        })\n",
    "        \n",
    "        valid_acc = sess.run('accuracy:0', feed_dict={\n",
    "            'features:0': mnist.validation.images,\n",
    "            'targets:0': mnist.validation.labels\n",
    "        })\n",
    "            \n",
    "        print(\"Epoch %03d | AvgCostL %.3f\" % (epoch + 1, avg_cost/(i + 1)), end=\"\")\n",
    "        print(\"| Acuracia em Treino/Validacao: %.3f/%.3f\" % (train_acc, valid_acc))\n",
    "        \n",
    "    test_acc = sess.run('accuracy:0', feed_dict={\n",
    "        'features:0': mnist.test.images,\n",
    "        'targets:0': mnist.test.labels\n",
    "    })\n",
    "    print(\"Acuracia em teste %.3f\" %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
